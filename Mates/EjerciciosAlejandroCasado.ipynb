{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f25c64d",
   "metadata": {},
   "source": [
    "# Ejercicios evaluables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b243c44",
   "metadata": {},
   "source": [
    "### *EJERCICIO 1*\n",
    "\n",
    "Tal y como ya hemos visto en clase, la variedad de herramientas proporcionadas por el\n",
    "√°lgebra lineal son cruciales para desarrollar y fundamentar las bases de una variedad de\n",
    "t√©cnicas relacionadas con el aprendizaje autom√°tico. Con ella, podemos describir el proceso\n",
    "de propagaci√≥n hacia adelante en una red neuronal, identificar m√≠nimos locales en funciones\n",
    "multivariables (crucial para el proceso de retropropagaci√≥n) o la descripci√≥n y empleo de\n",
    "m√©todos de reducci√≥n de la dimensionalidad, como el an√°lisis de componentes principales\n",
    "(PCA), entre muchas otras aplicaciones.\n",
    "Cuando trabajamos en la pr√°ctica dentro de este √°mbito, la cantidad de datos que manejamos\n",
    "puede ser muy grande, por lo que es especialmente importante emplear algoritmos eficientes\n",
    "y optimizados para reducir el coste computacional en la medida de lo posible. Por todo ello,\n",
    "el objetivo de este ejercicio es el de ilustrar las diferentes alternativas que pueden existir\n",
    "para realizar un proceso relacionado con el √°lgebra lineal y el impacto que puede tener cada\n",
    "variante en t√©rminos del coste computacional del mismo. En este caso en particular, y a modo\n",
    "de ilustraci√≥n, nos centraremos en el c√°lculo del determinante de una matriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b196a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importacion de librerias\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57c2343",
   "metadata": {},
   "source": [
    "1. Implementa una funci√≥n, determinante recursivo, que obtenga el determinante de una matriz cuadrada utilizando la definici√≥n recursiva de Laplace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b906daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determinante_recursivo(A):\n",
    "    A = np.array(A)\n",
    "    n = A.shape[0]\n",
    "\n",
    "    # Caso base: matriz 1x1\n",
    "    if n == 1:\n",
    "        return A[0, 0]\n",
    "\n",
    "    # Caso base: matriz 2x2\n",
    "    if n == 2:\n",
    "        return A[0,0]*A[1,1] - A[0,1]*A[1,0]\n",
    "\n",
    "    # Caso general\n",
    "    det = 0\n",
    "    for j in range(n):\n",
    "        # Eliminar primera fila y j-√©sima columna\n",
    "        submatriz = np.delete(np.delete(A, 0, axis=0), j, axis=1)\n",
    "        cofactor = (-1) ** j * A[0, j] * determinante_recursivo(submatriz)\n",
    "        det += cofactor\n",
    "\n",
    "    return det"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ae84ca",
   "metadata": {},
   "source": [
    "2. Si A es una matriz cuadrada n√ón y triangular (superior o inferior, es decir, con entradas nulas por debajo o por encima de la diagonal, respectivamente), ¬øexiste alguna forma de calcular de forma directa y sencilla su determinante? Justif√≠quese la respuesta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded58965",
   "metadata": {},
   "source": [
    "    S√≠, la forma m√°s r√°pida consiste en calcular el producto de los elementos de la diagonal principal. Esto se debe a que, en una matriz triangular (superior o inferior), todos los determinantes menores involucrados en la expansi√≥n de Laplace que no pasan por la diagonal contienen ceros, lo que anula sus contribuciones. As√≠, el √∫nico camino no anulado por ceros es el producto de los elementos diagonales, que coincide con el determinante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56d12c3",
   "metadata": {},
   "source": [
    "3. Determ√≠nese de forma justificada c√≥mo alteran el determinante de una\n",
    "matriz n √ó n las dos operaciones elementales siguientes:\n",
    "\n",
    "- Intercambiar una fila (o columna) por otra fila (o columna).\n",
    "\n",
    "- Sumar a una fila (o columna) otra fila (o columna) multiplicada por un escalar Œ±.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b191a0",
   "metadata": {},
   "source": [
    "    Al intercambiar dos filas o columnas de una matriz, el determinante de la misma cambia de signo. Esto concluye con que:\n",
    "    - Si el n√∫mero de modificaciones que realizamos es par (n % 2 == 0): El signo original se mantiene\n",
    "    - Si el n√∫mero de modificaciones que realizamos es impar (n % 2 == 1): El signo cambia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71c38bd",
   "metadata": {},
   "source": [
    "    Al sumar a una fila o columna otra fila o columna multiplicada por un escalar Œ±, no alteraremos el determinante, pues este es lineal respecto a cada fila o columna por separado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5617cecf",
   "metadata": {},
   "source": [
    "4.  Investiga sobre el m√©todo de eliminaci√≥n de Gauss con pivoteo parcial e\n",
    "implem√©ntalo para escalonar una matriz (es decir, convertirla en una matriz triangular\n",
    "inferior) a partir de las operaciones elementales descritas en el apartado anterior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8733581e",
   "metadata": {},
   "source": [
    "    La eliminaci√≥n de Gauss consiste en aplicar operaciones elementales para transformar una matriz en una forma triangular (superior o inferior), lo que facilita resolver sistemas lineales o calcular determinantes.\n",
    "    El pivote es considerado el valor de la diagonal que se va a usar para anular los elementos superiores o inferiores. En la pr√°ctica, si el pivote es muy peque√±o o cero, pueden aparecer errores num√©ricos o divisiones por cero. El pivoteo parcial soluciona esto intercambiando la fila actual con la que tiene el valor absoluto mayor en la columna del pivote."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5f4320",
   "metadata": {},
   "source": [
    "5.  ¬øC√≥mo se podr√≠a calcular el determinante de una matriz haciendo beneficio\n",
    "de la estrategia anterior y del efecto de aplicar las operaciones elementales pertinentes?\n",
    "Implementa una nueva funci√≥n, determinante gauss, que calcule el determinante de\n",
    "una matriz utilizando eliminaci√≥n gaussiana.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "513a5f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determinante_gauss(A):\n",
    "    A = A.astype(float).copy() # Copiar para no modificar la original\n",
    "    n = A.shape[0]\n",
    "    intercambios_cont = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        # Pivoteo parcial: buscar la fila con m√°ximo valor absoluto en columna i desde fila i\n",
    "        max_fila = i + np.argmax(np.abs(A[i:,i]))\n",
    "\n",
    "        #Caso para matriz singular (Det = 0)\n",
    "        if A[max_fila, i] == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Intercambiar filas si es necesario\n",
    "        if max_fila != i:\n",
    "            A[[i, max_fila]] = A[[max_fila, i]]\n",
    "            intercambios_cont += 1\n",
    "\n",
    "        # Eliminar elementos debajo del pivote\n",
    "        for j in range(i + 1, n):\n",
    "            factor = A[j, i] / A[i, i]\n",
    "            A[j, i:] -= factor * A[i, i:]\n",
    "            A[j, i] = 0  # Para evitar errores num√©ricos\n",
    "        \n",
    "    # Producto de la diagonal principal\n",
    "    det = np.prod(np.diag(A))\n",
    "    \n",
    "    # Ajustar signo seg√∫n n√∫mero de intercambios\n",
    "    if intercambios_cont % 2 != 0:\n",
    "        det = -det\n",
    "\n",
    "\n",
    "    return det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "93533a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determinante calculado con eliminaci√≥n gaussiana: 46.00000000000006\n",
      "Determinante con numpy.linalg.det: 46.00000000000004\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [1, 8, 7],\n",
    "    [4, 10, 9],\n",
    "    [7, 4, 2]\n",
    "])\n",
    "\n",
    "print(\"Determinante calculado con eliminaci√≥n gaussiana:\", determinante_gauss(A))\n",
    "print(\"Determinante con numpy.linalg.det:\", np.linalg.det(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861876c5",
   "metadata": {},
   "source": [
    "    Como se puede comprobar, se han obtenido resultados similares aplicando la funci√≥n esencial de Numpy y nuestra funci√≥n eliminaci√≥n gaussiana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef92d547",
   "metadata": {},
   "source": [
    "6.  Obt√©n la complejidad computacional asociada al c√°lculo del determinante\n",
    "con la definici√≥n recursiva y con el m√©todo de eliminaci√≥n de Gauss con pivoteo parcial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83db944",
   "metadata": {},
   "source": [
    "    Por un lado, la definici√≥n recursiva consiste en descomponer el determinante de una matriz n x n en la suma de determinantes de matrices (n-1) x (n-1). por ello, en cada nivel se realizan 'n' llamadas para calcular determinantes, lo que genera una complejidad exponencial O(n!), la cual no es viable para grandes cantidades de datos.\n",
    "\n",
    "    En cambio, el m√©todo de la eliminaci√≥n gaussiana transforma la matriz en forma triangular mediante operaciones elementales. Por ello, el n√∫mero total de operaciones aritm√©ticas est√° en el orden de O(2/3*n^3). M√°s formalmente, se considera que tiene complejidad c√∫bica: O(n^3). Por lo tanto, el c√°lculo del determinante con este m√©todo es mucho m√°s eficiente y pr√°ctico para matrices grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3533a4",
   "metadata": {},
   "source": [
    "\n",
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e4c0a9",
   "metadata": {},
   "source": [
    "### *EJERCICIO 2*\n",
    "En este ejercicio trabajaremos con el m¬¥etodo de descenso de gradiente, el cual constituye\n",
    "otra herramienta crucial, en esta ocasi¬¥on de la rama del c¬¥alculo, para el proceso de retropropagaci¬¥on asociado al entrenamiento de una red neuronal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43a2514",
   "metadata": {},
   "source": [
    "1. Progr√°mese en Python el **m√©todo de descenso de gradiente** para funciones de \\( n \\) variables. La funci√≥n debe recibir como par√°metros de entrada:\n",
    "\n",
    "-  El gradiente de la funci√≥n que se desea minimizar el  ‚àáf  (puede ser otra funci√≥n previamente implementada, llamada `grad_f`, que reciba un vector ‚Äî el punto donde se calcula el gradiente ‚Äî y devuelva otro vector ‚Äî el gradiente en dicho punto).\n",
    "-  Un valor inicial  x0 ‚àà R n  (almacenado como un vector de \\( n \\) componentes).\n",
    "-  El ratio de aprendizaje Œ≥ (constante para cada iteraci√≥n).\n",
    "-  Un par√°metro de tolerancia `tol` para finalizar el proceso cuando ‚à•‚àáf(x)‚à•2 < `tol`).\n",
    "-  Un n√∫mero m√°ximo de iteraciones `maxit` para evitar ejecuciones indefinidas en caso de divergencia o convergencia lenta.\n",
    "\n",
    "La salida de la funci√≥n debe ser la aproximaci√≥n de x que cumple f(x) ‚âà 0, correspondiente a la √∫ltima iteraci√≥n realizada en el m√©todo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dc114fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descenso_gradiente(grad_f, x0, gamma, tol, maxit):\n",
    "    \"\"\"\n",
    "    M√©todo de descenso de gradiente para funciones de n variables.\n",
    "\n",
    "    Par√°metros:\n",
    "    grad_f : funci√≥n que calcula el gradiente en un punto (vector).\n",
    "    x0 : numpy array, punto inicial (vector).\n",
    "    gamma : float, ratio de aprendizaje.\n",
    "    tol : float, tolerancia para la norma del gradiente.\n",
    "    maxit : int, n√∫mero m√°ximo de iteraciones.\n",
    "\n",
    "    Retorna:\n",
    "    x : numpy array, aproximaci√≥n al m√≠nimo de la funci√≥n.\n",
    "    \"\"\"\n",
    "    x = x0.copy()\n",
    "    \n",
    "    for _ in range(maxit):\n",
    "        grad = grad_f(x)\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "        x = x - gamma * grad\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8980fa",
   "metadata": {},
   "source": [
    "2. Sea la funci√≥n f : R ‚Üí R dada por\n",
    "\n",
    "##### f(x) = 3x^4 + 4x^3 - 12x^2 + 7\n",
    "\n",
    "---\n",
    "\n",
    "i) Aplica el m√©todo de descenso de gradiente con:\n",
    "\n",
    "- x_0 = 3\n",
    "- Œ≥ = 0.001\n",
    "- `tol` = 1 x 10^{-12}\n",
    "- `maxit` = 10^5\n",
    "\n",
    "---\n",
    "\n",
    "ii) Aplica el m√©todo con:\n",
    "\n",
    "- x_0 = 3\n",
    "- Œ≥ = 0.01\n",
    "- `tol` = 1 x 10^{-12}\n",
    "- `maxit` = 10^5\n",
    "\n",
    "---\n",
    "\n",
    "Compara e interpreta los resultados de i) y ii) en relaci√≥n con los m√≠nimos locales anal√≠ticos y la influencia del ratio de aprendizaje Œ≥.\n",
    "\n",
    "---\n",
    "\n",
    "iii) Aplica el m√©todo con:\n",
    "\n",
    "- x_0 = 3\n",
    "- Œ≥ = 0.1\n",
    "- `tol` = 1 x 10^{-12}\n",
    "- `maxit` = 10^5\n",
    "\n",
    "Interpreta el resultado.\n",
    "\n",
    "---\n",
    "\n",
    "iv) Aplica el m√©todo con:\n",
    "\n",
    "- x_0 = 0\n",
    "- Œ≥ = 0.001\n",
    "- `tol` = 1 \\times 10^{-12}\n",
    "- `maxit` = 10^5\n",
    "\n",
    "Interpreta el resultado y comp√°ralo con el an√°lisis de f. ¬øEs un resultado deseable? ¬øPor qu√©? ¬øA qu√© se debe este fen√≥meno?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e68379a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i) gamma=0.001 -> m√≠nimo en x = 1.0000000000 tras 832 iteraciones, f(x) = 2.0000000000\n",
      "\n",
      "ii) gamma=0.01 -> m√≠nimo en x = -2.0000000000 tras 32 iteraciones, f(x) = -25.0000000000\n",
      "\n",
      "iii) El caso genera un error de Overflow al crecer demasiado el valor de x durante las iteraciones y Python no puede manejar el resultado pues se sale del rango\n",
      "\n",
      "iv) x0=0, gamma=0.001 -> m√≠nimo en x = 0.0000000000 tras 1 iteraciones, f(x) = 7.0000000000\n"
     ]
    }
   ],
   "source": [
    "# Definimos la funci√≥n f(x)\n",
    "def f(x):\n",
    "    return 3*x**4 + 4*x**3 - 12*x**2 + 7\n",
    "\n",
    "def grad_f(x):\n",
    "    return 12*x**3 + 12*x**2 - 24*x\n",
    "\n",
    "# Implementaci√≥n del descenso de gradiente para funciones R->R, de una sola variable\n",
    "def descenso_gradiente_1d(grad_f, x0, gamma, tol, maxit):\n",
    "    x = x0\n",
    "    for i in range(int(maxit)):\n",
    "        grad = grad_f(x)\n",
    "        if abs(grad) < tol:\n",
    "            break\n",
    "        x = x - gamma * grad\n",
    "    return x, i+1\n",
    "\n",
    "# Par√°metros\n",
    "tol = 1e-12\n",
    "maxit = 1e5\n",
    "\n",
    "# i) gamma = 0.001\n",
    "x0 = 3\n",
    "gamma = 0.001\n",
    "x_min_a, iter_a = descenso_gradiente_1d(grad_f, x0, gamma, tol, maxit)\n",
    "print(f\"i) gamma={gamma} -> m√≠nimo en x = {x_min_a:.10f} tras {iter_a} iteraciones, f(x) = {f(x_min_a):.10f}\\n\")\n",
    "\n",
    "# ii) gamma = 0.01\n",
    "gamma = 0.01\n",
    "x_min_b, iter_b = descenso_gradiente_1d(grad_f, x0, gamma, tol, maxit)\n",
    "print(f\"ii) gamma={gamma} -> m√≠nimo en x = {x_min_b:.10f} tras {iter_b} iteraciones, f(x) = {f(x_min_b):.10f}\\n\")\n",
    "\n",
    "# iii) gamma = 0.1\n",
    "gamma = 0.1\n",
    "#x_min_d, iter_d = descenso_gradiente_1d(grad_f, x0, gamma, tol, maxit)\n",
    "#print(f\"d) gamma={gamma} -> m√≠nimo en x = {x_min_d:.10f} tras {iter_d} iteraciones, f(x) = {f(x_min_d):.10f}\")\n",
    "print(f\"iii) El caso genera un error de Overflow al crecer demasiado el valor de x durante las iteraciones y Python no puede manejar el resultado pues se sale del rango\\n\")\n",
    "\n",
    "# iv) x0=0, gamma=0.001\n",
    "x0 = 0\n",
    "gamma = 0.001\n",
    "x_min_e, iter_e = descenso_gradiente_1d(grad_f, x0, gamma, tol, maxit)\n",
    "print(f\"iv) x0={x0}, gamma={gamma} -> m√≠nimo en x = {x_min_e:.10f} tras {iter_e} iteraciones, f(x) = {f(x_min_e):.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd3005",
   "metadata": {},
   "source": [
    "### Interpretaci√≥n\n",
    "\n",
    "- La elecci√≥n del par√°metro gamma es fundamental, como se observa en los tres primeros casos. En el primero, un valor peque√±o de gamma conduce a la convergencia hacia un m√≠nimo local, aunque con un n√∫mero elevado de iteraciones (832). En el segundo caso, un gamma mayor permite alcanzar un mejor m√≠nimo (con valor de funci√≥n m√°s bajo, -25 frente a 2) en menos iteraciones (32). Sin embargo, como muestra el tercer caso, un gamma demasiado grande puede provocar que el m√©todo diverja, generando errores debido a los saltos excesivamente grandes en el proceso de actualizaci√≥n.\n",
    "\n",
    "- Por otro lado, en la cuarta opci√≥n se modifica el punto inicial ùë•0=0. En este caso, el m√©todo converge en una √∫nica iteraci√≥n a un punto donde la derivada es cero, pero que no corresponde a un m√≠nimo local. Esto evidencia la importancia de elegir un buen punto de partida para el m√©todo de descenso de gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abded42a",
   "metadata": {},
   "source": [
    "3. Sea la funci√≥n f : R^2 ‚Üí R dada por\n",
    "\n",
    "##### g(x,y) = x2 + y3 + 3xy + 1\n",
    "\n",
    "---\n",
    "\n",
    "i) Aplica el m√©todo de descenso de gradiente sobre la funci√≥n g(x, y) con los siguientes par√°metros:\n",
    "\n",
    "Punto inicial: x0 = (-1, 1)\n",
    "\n",
    "Ratio de aprendizaje: gamma = 0.01\n",
    "\n",
    "Tolerancia: tol = 1e-12\n",
    "\n",
    "N√∫mero m√°ximo de iteraciones: maxit = 1e5\n",
    "\n",
    "---\n",
    "\n",
    "ii) ¬øQu√© ocurre si ahora partimos de x0 = (0, 0)? ¬øSe obtiene un resultado deseable?\n",
    "\n",
    "---\n",
    "\n",
    "iii) Real√≠cese el estudio anal√≠tico de la funci√≥n y util√≠cese para explicar y contrastar los resultados obtenidos en los dos apartados anteriores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c22563f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i) m√≠nimo en x = [-2.25  1.5 ] tras 3140 iteraciones, g(x) = -0.6875000000\n",
      "ii) m√≠nimo en x = [0. 0.] tras 1 iteraciones, g(x) = 1.0000000000\n",
      "\n",
      "Estudio anal√≠tico:\n",
      "Puntos cr√≠ticos cumplen:\n",
      "2x + 3y = 0\n",
      "3y^2 + 3x = 0\n",
      "\n",
      "De la primera: x = -3/2 y\n",
      "Sustituyendo en la segunda:\n",
      "3 y^2 + 3 (-3/2 y) = 3 y^2 - (9/2) y = 0\n",
      "3 y (y - 3/2) = 0\n",
      "\n",
      "Por tanto, y=0 o y=3/2\n",
      "Si y=0 -> x=0\n",
      "Si y=3/2 -> x = -9/4 = -2.25\n",
      "\n",
      "Puntos cr√≠ticos: (0,0) y (-2.25, 1.5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#volvemos a realizar la funcion descenso de gradiente (se podr√≠a usar la anterior)\n",
    "def descenso_gradiente_ej3(grad_f, x0, gamma, tol, maxit):\n",
    "    x = np.array(x0, dtype=float)\n",
    "    for i in range(maxit):\n",
    "        grad = grad_f(x)\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "        x = x - gamma * grad\n",
    "    return x, i+1\n",
    "\n",
    "# Funci√≥n g(x,y)\n",
    "def g(x):\n",
    "    return x[0]**2 + x[1]**3 + 3*x[0]*x[1] + 1\n",
    "\n",
    "# Gradiente de g\n",
    "def grad_g(x):\n",
    "    dx = 2*x[0] + 3*x[1]\n",
    "    dy = 3*x[1]**2 + 3*x[0]\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "# Par√°metros\n",
    "gamma = 0.01\n",
    "tol = 1e-12\n",
    "maxit = 1e5\n",
    "\n",
    "# i) Punto inicial (-1,1)\n",
    "x0_i = [-1, 1]\n",
    "x_min_i, iter_i = descenso_gradiente_ej3(grad_g, x0_i, gamma, tol, int(maxit))\n",
    "print(f\"i) m√≠nimo en x = {x_min_i} tras {iter_i} iteraciones, g(x) = {g(x_min_i):.10f}\")\n",
    "\n",
    "# ii) Punto inicial (0,0)\n",
    "x0_ii = [0, 0]\n",
    "x_min_ii, iter_ii = descenso_gradiente_ej3(grad_g, x0_ii, gamma, tol, int(maxit))\n",
    "print(f\"ii) m√≠nimo en x = {x_min_ii} tras {iter_ii} iteraciones, g(x) = {g(x_min_ii):.10f}\")\n",
    "\n",
    "# iii) Estudio anal√≠tico\n",
    "print(\"\"\"\n",
    "Estudio anal√≠tico:\n",
    "Puntos cr√≠ticos cumplen:\n",
    "2x + 3y = 0\n",
    "3y^2 + 3x = 0\n",
    "\n",
    "De la primera: x = -3/2 y\n",
    "Sustituyendo en la segunda:\n",
    "3 y^2 + 3 (-3/2 y) = 3 y^2 - (9/2) y = 0\n",
    "3 y (y - 3/2) = 0\n",
    "\n",
    "Por tanto, y=0 o y=3/2\n",
    "Si y=0 -> x=0\n",
    "Si y=3/2 -> x = -9/4 = -2.25\n",
    "\n",
    "Puntos cr√≠ticos: (0,0) y (-2.25, 1.5)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249c9088",
   "metadata": {},
   "source": [
    "### Interpretaci√≥n\n",
    "\n",
    "- En el primer caso, partiendo de \\((-1,1)\\), el m√©todo converge a un m√≠nimo local, indicando que el punto inicial est√° dentro de la zona de atracci√≥n de dicho m√≠nimo.\n",
    "- En el segundo caso, partiendo de \\((0,0)\\), el m√©todo puede converger a un punto estacionario que no es m√≠nimo (puede ser un punto de silla o m√°ximo) o no converger correctamente, ya que la derivada es cero pero no se trata de un m√≠nimo.\n",
    "- Por ello, la elecci√≥n del punto inicial es fundamental para el √©xito del m√©todo de descenso de gradiente. Adem√°s, el ratio de aprendizaje influye en la velocidad y estabilidad de la convergencia, como se ha observado en ejercicios anteriores.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
