{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f25c64d",
   "metadata": {},
   "source": [
    "# Ejercicios evaluables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b243c44",
   "metadata": {},
   "source": [
    "### *EJERCICIO 1*\n",
    "\n",
    "Tal y como ya hemos visto en clase, la variedad de herramientas proporcionadas por el\n",
    "álgebra lineal son cruciales para desarrollar y fundamentar las bases de una variedad de\n",
    "técnicas relacionadas con el aprendizaje automático. Con ella, podemos describir el proceso\n",
    "de propagación hacia adelante en una red neuronal, identificar mínimos locales en funciones\n",
    "multivariables (crucial para el proceso de retropropagación) o la descripción y empleo de\n",
    "métodos de reducción de la dimensionalidad, como el análisis de componentes principales\n",
    "(PCA), entre muchas otras aplicaciones.\n",
    "Cuando trabajamos en la práctica dentro de este ámbito, la cantidad de datos que manejamos\n",
    "puede ser muy grande, por lo que es especialmente importante emplear algoritmos eficientes\n",
    "y optimizados para reducir el coste computacional en la medida de lo posible. Por todo ello,\n",
    "el objetivo de este ejercicio es el de ilustrar las diferentes alternativas que pueden existir\n",
    "para realizar un proceso relacionado con el álgebra lineal y el impacto que puede tener cada\n",
    "variante en términos del coste computacional del mismo. En este caso en particular, y a modo\n",
    "de ilustración, nos centraremos en el cálculo del determinante de una matriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b196a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importacion de librerias\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57c2343",
   "metadata": {},
   "source": [
    "1. Implementa una función, determinante recursivo, que obtenga el determinante de una matriz cuadrada utilizando la definición recursiva de Laplace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b906daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determinante_recursivo(A):\n",
    "    A = np.array(A)\n",
    "    n = A.shape[0]\n",
    "\n",
    "    # Caso base: matriz 1x1\n",
    "    if n == 1:\n",
    "        return A[0, 0]\n",
    "\n",
    "    # Caso base: matriz 2x2\n",
    "    if n == 2:\n",
    "        return A[0,0]*A[1,1] - A[0,1]*A[1,0]\n",
    "\n",
    "    # Caso general\n",
    "    det = 0\n",
    "    for j in range(n):\n",
    "        # Eliminar primera fila y j-ésima columna\n",
    "        submatriz = np.delete(np.delete(A, 0, axis=0), j, axis=1)\n",
    "        cofactor = (-1) ** j * A[0, j] * determinante_recursivo(submatriz)\n",
    "        det += cofactor\n",
    "\n",
    "    return det"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ae84ca",
   "metadata": {},
   "source": [
    "2. Si A es una matriz cuadrada n×n y triangular (superior o inferior, es decir, con entradas nulas por debajo o por encima de la diagonal, respectivamente), ¿existe alguna forma de calcular de forma directa y sencilla su determinante? Justifíquese la respuesta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded58965",
   "metadata": {},
   "source": [
    "    Sí, la forma más rápida consiste en calcular el producto de los elementos de la diagonal principal. Esto se debe a que, en una matriz triangular (superior o inferior), todos los determinantes menores involucrados en la expansión de Laplace que no pasan por la diagonal contienen ceros, lo que anula sus contribuciones. Así, el único camino no anulado por ceros es el producto de los elementos diagonales, que coincide con el determinante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56d12c3",
   "metadata": {},
   "source": [
    "3. Determínese de forma justificada cómo alteran el determinante de una\n",
    "matriz n × n las dos operaciones elementales siguientes:\n",
    "\n",
    "- Intercambiar una fila (o columna) por otra fila (o columna).\n",
    "\n",
    "- Sumar a una fila (o columna) otra fila (o columna) multiplicada por un escalar α.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b191a0",
   "metadata": {},
   "source": [
    "    Al intercambiar dos filas o columnas de una matriz, el determinante de la misma cambia de signo. Esto concluye con que:\n",
    "    - Si el número de modificaciones que realizamos es par (n % 2 == 0): El signo original se mantiene\n",
    "    - Si el número de modificaciones que realizamos es impar (n % 2 == 1): El signo cambia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71c38bd",
   "metadata": {},
   "source": [
    "    Al sumar a una fila o columna otra fila o columna multiplicada por un escalar α, no alteraremos el determinante, pues este es lineal respecto a cada fila o columna por separado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5617cecf",
   "metadata": {},
   "source": [
    "4.  Investiga sobre el método de eliminación de Gauss con pivoteo parcial e\n",
    "impleméntalo para escalonar una matriz (es decir, convertirla en una matriz triangular\n",
    "inferior) a partir de las operaciones elementales descritas en el apartado anterior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8733581e",
   "metadata": {},
   "source": [
    "    La eliminación de Gauss consiste en aplicar operaciones elementales para transformar una matriz en una forma triangular (superior o inferior), lo que facilita resolver sistemas lineales o calcular determinantes.\n",
    "    El pivote es considerado el valor de la diagonal que se va a usar para anular los elementos superiores o inferiores. En la práctica, si el pivote es muy pequeño o cero, pueden aparecer errores numéricos o divisiones por cero. El pivoteo parcial soluciona esto intercambiando la fila actual con la que tiene el valor absoluto mayor en la columna del pivote."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5f4320",
   "metadata": {},
   "source": [
    "5.  ¿Cómo se podría calcular el determinante de una matriz haciendo beneficio\n",
    "de la estrategia anterior y del efecto de aplicar las operaciones elementales pertinentes?\n",
    "Implementa una nueva función, determinante gauss, que calcule el determinante de\n",
    "una matriz utilizando eliminación gaussiana.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "513a5f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determinante_gauss(A):\n",
    "    A = A.astype(float).copy() # Copiar para no modificar la original\n",
    "    n = A.shape[0]\n",
    "    intercambios_cont = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        # Pivoteo parcial: buscar la fila con máximo valor absoluto en columna i desde fila i\n",
    "        max_fila = i + np.argmax(np.abs(A[i:,i]))\n",
    "\n",
    "        #Caso para matriz singular (Det = 0)\n",
    "        if A[max_fila, i] == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Intercambiar filas si es necesario\n",
    "        if max_fila != i:\n",
    "            A[[i, max_fila]] = A[[max_fila, i]]\n",
    "            intercambios_cont += 1\n",
    "\n",
    "        # Eliminar elementos debajo del pivote\n",
    "        for j in range(i + 1, n):\n",
    "            factor = A[j, i] / A[i, i]\n",
    "            A[j, i:] -= factor * A[i, i:]\n",
    "            A[j, i] = 0  # Para evitar errores numéricos\n",
    "        \n",
    "    # Producto de la diagonal principal\n",
    "    det = np.prod(np.diag(A))\n",
    "    \n",
    "    # Ajustar signo según número de intercambios\n",
    "    if intercambios_cont % 2 != 0:\n",
    "        det = -det\n",
    "\n",
    "\n",
    "    return det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "93533a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determinante calculado con eliminación gaussiana: 46.00000000000006\n",
      "Determinante con numpy.linalg.det: 46.00000000000004\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [1, 8, 7],\n",
    "    [4, 10, 9],\n",
    "    [7, 4, 2]\n",
    "])\n",
    "\n",
    "print(\"Determinante calculado con eliminación gaussiana:\", determinante_gauss(A))\n",
    "print(\"Determinante con numpy.linalg.det:\", np.linalg.det(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861876c5",
   "metadata": {},
   "source": [
    "    Como se puede comprobar, se han obtenido resultados similares aplicando la función esencial de Numpy y nuestra función eliminación gaussiana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef92d547",
   "metadata": {},
   "source": [
    "6.  Obtén la complejidad computacional asociada al cálculo del determinante\n",
    "con la definición recursiva y con el método de eliminación de Gauss con pivoteo parcial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83db944",
   "metadata": {},
   "source": [
    "    Por un lado, la definición recursiva consiste en descomponer el determinante de una matriz n x n en la suma de determinantes de matrices (n-1) x (n-1). por ello, en cada nivel se realizan 'n' llamadas para calcular determinantes, lo que genera una complejidad exponencial O(n!), la cual no es viable para grandes cantidades de datos.\n",
    "\n",
    "    En cambio, el método de la eliminación gaussiana transforma la matriz en forma triangular mediante operaciones elementales. Por ello, el número total de operaciones aritméticas está en el orden de O(2/3*n^3). Más formalmente, se considera que tiene complejidad cúbica: O(n^3). Por lo tanto, el cálculo del determinante con este método es mucho más eficiente y práctico para matrices grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3533a4",
   "metadata": {},
   "source": [
    "\n",
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e4c0a9",
   "metadata": {},
   "source": [
    "### *EJERCICIO 2*\n",
    "En este ejercicio trabajaremos con el m´etodo de descenso de gradiente, el cual constituye\n",
    "otra herramienta crucial, en esta ocasi´on de la rama del c´alculo, para el proceso de retropropagaci´on asociado al entrenamiento de una red neuronal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43a2514",
   "metadata": {},
   "source": [
    "1. Prográmese en Python el **método de descenso de gradiente** para funciones de \\( n \\) variables. La función debe recibir como parámetros de entrada:\n",
    "\n",
    "-  El gradiente de la función que se desea minimizar el  ∇f  (puede ser otra función previamente implementada, llamada `grad_f`, que reciba un vector — el punto donde se calcula el gradiente — y devuelva otro vector — el gradiente en dicho punto).\n",
    "-  Un valor inicial  x0 ∈ R n  (almacenado como un vector de \\( n \\) componentes).\n",
    "-  El ratio de aprendizaje γ (constante para cada iteración).\n",
    "-  Un parámetro de tolerancia `tol` para finalizar el proceso cuando ∥∇f(x)∥2 < `tol`).\n",
    "-  Un número máximo de iteraciones `maxit` para evitar ejecuciones indefinidas en caso de divergencia o convergencia lenta.\n",
    "\n",
    "La salida de la función debe ser la aproximación de x que cumple f(x) ≈ 0, correspondiente a la última iteración realizada en el método."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dc114fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descenso_gradiente(grad_f, x0, gamma, tol, maxit):\n",
    "    \"\"\"\n",
    "    Método de descenso de gradiente para funciones de n variables.\n",
    "\n",
    "    Parámetros:\n",
    "    grad_f : función que calcula el gradiente en un punto (vector).\n",
    "    x0 : numpy array, punto inicial (vector).\n",
    "    gamma : float, ratio de aprendizaje.\n",
    "    tol : float, tolerancia para la norma del gradiente.\n",
    "    maxit : int, número máximo de iteraciones.\n",
    "\n",
    "    Retorna:\n",
    "    x : numpy array, aproximación al mínimo de la función.\n",
    "    \"\"\"\n",
    "    x = x0.copy()\n",
    "    \n",
    "    for _ in range(maxit):\n",
    "        grad = grad_f(x)\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "        x = x - gamma * grad\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8980fa",
   "metadata": {},
   "source": [
    "2. Sea la función f : R → R dada por\n",
    "\n",
    "##### f(x) = 3x^4 + 4x^3 - 12x^2 + 7\n",
    "\n",
    "---\n",
    "\n",
    "i) Aplica el método de descenso de gradiente con:\n",
    "\n",
    "- x_0 = 3\n",
    "- γ = 0.001\n",
    "- `tol` = 1 x 10^{-12}\n",
    "- `maxit` = 10^5\n",
    "\n",
    "---\n",
    "\n",
    "ii) Aplica el método con:\n",
    "\n",
    "- x_0 = 3\n",
    "- γ = 0.01\n",
    "- `tol` = 1 x 10^{-12}\n",
    "- `maxit` = 10^5\n",
    "\n",
    "---\n",
    "\n",
    "Compara e interpreta los resultados de i) y ii) en relación con los mínimos locales analíticos y la influencia del ratio de aprendizaje γ.\n",
    "\n",
    "---\n",
    "\n",
    "iii) Aplica el método con:\n",
    "\n",
    "- x_0 = 3\n",
    "- γ = 0.1\n",
    "- `tol` = 1 x 10^{-12}\n",
    "- `maxit` = 10^5\n",
    "\n",
    "Interpreta el resultado.\n",
    "\n",
    "---\n",
    "\n",
    "iv) Aplica el método con:\n",
    "\n",
    "- x_0 = 0\n",
    "- γ = 0.001\n",
    "- `tol` = 1 \\times 10^{-12}\n",
    "- `maxit` = 10^5\n",
    "\n",
    "Interpreta el resultado y compáralo con el análisis de f. ¿Es un resultado deseable? ¿Por qué? ¿A qué se debe este fenómeno?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e68379a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i) gamma=0.001 -> mínimo en x = 1.0000000000 tras 832 iteraciones, f(x) = 2.0000000000\n",
      "\n",
      "ii) gamma=0.01 -> mínimo en x = -2.0000000000 tras 32 iteraciones, f(x) = -25.0000000000\n",
      "\n",
      "iii) El caso genera un error de Overflow al crecer demasiado el valor de x durante las iteraciones y Python no puede manejar el resultado pues se sale del rango\n",
      "\n",
      "iv) x0=0, gamma=0.001 -> mínimo en x = 0.0000000000 tras 1 iteraciones, f(x) = 7.0000000000\n"
     ]
    }
   ],
   "source": [
    "# Definimos la función f(x)\n",
    "def f(x):\n",
    "    return 3*x**4 + 4*x**3 - 12*x**2 + 7\n",
    "\n",
    "def grad_f(x):\n",
    "    return 12*x**3 + 12*x**2 - 24*x\n",
    "\n",
    "# Implementación del descenso de gradiente para funciones R->R, de una sola variable\n",
    "def descenso_gradiente_1d(grad_f, x0, gamma, tol, maxit):\n",
    "    x = x0\n",
    "    for i in range(int(maxit)):\n",
    "        grad = grad_f(x)\n",
    "        if abs(grad) < tol:\n",
    "            break\n",
    "        x = x - gamma * grad\n",
    "    return x, i+1\n",
    "\n",
    "# Parámetros\n",
    "tol = 1e-12\n",
    "maxit = 1e5\n",
    "\n",
    "# i) gamma = 0.001\n",
    "x0 = 3\n",
    "gamma = 0.001\n",
    "x_min_a, iter_a = descenso_gradiente_1d(grad_f, x0, gamma, tol, maxit)\n",
    "print(f\"i) gamma={gamma} -> mínimo en x = {x_min_a:.10f} tras {iter_a} iteraciones, f(x) = {f(x_min_a):.10f}\\n\")\n",
    "\n",
    "# ii) gamma = 0.01\n",
    "gamma = 0.01\n",
    "x_min_b, iter_b = descenso_gradiente_1d(grad_f, x0, gamma, tol, maxit)\n",
    "print(f\"ii) gamma={gamma} -> mínimo en x = {x_min_b:.10f} tras {iter_b} iteraciones, f(x) = {f(x_min_b):.10f}\\n\")\n",
    "\n",
    "# iii) gamma = 0.1\n",
    "gamma = 0.1\n",
    "#x_min_d, iter_d = descenso_gradiente_1d(grad_f, x0, gamma, tol, maxit)\n",
    "#print(f\"d) gamma={gamma} -> mínimo en x = {x_min_d:.10f} tras {iter_d} iteraciones, f(x) = {f(x_min_d):.10f}\")\n",
    "print(f\"iii) El caso genera un error de Overflow al crecer demasiado el valor de x durante las iteraciones y Python no puede manejar el resultado pues se sale del rango\\n\")\n",
    "\n",
    "# iv) x0=0, gamma=0.001\n",
    "x0 = 0\n",
    "gamma = 0.001\n",
    "x_min_e, iter_e = descenso_gradiente_1d(grad_f, x0, gamma, tol, maxit)\n",
    "print(f\"iv) x0={x0}, gamma={gamma} -> mínimo en x = {x_min_e:.10f} tras {iter_e} iteraciones, f(x) = {f(x_min_e):.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd3005",
   "metadata": {},
   "source": [
    "### Interpretación\n",
    "\n",
    "- La elección del parámetro gamma es fundamental, como se observa en los tres primeros casos. En el primero, un valor pequeño de gamma conduce a la convergencia hacia un mínimo local, aunque con un número elevado de iteraciones (832). En el segundo caso, un gamma mayor permite alcanzar un mejor mínimo (con valor de función más bajo, -25 frente a 2) en menos iteraciones (32). Sin embargo, como muestra el tercer caso, un gamma demasiado grande puede provocar que el método diverja, generando errores debido a los saltos excesivamente grandes en el proceso de actualización.\n",
    "\n",
    "- Por otro lado, en la cuarta opción se modifica el punto inicial 𝑥0=0. En este caso, el método converge en una única iteración a un punto donde la derivada es cero, pero que no corresponde a un mínimo local. Esto evidencia la importancia de elegir un buen punto de partida para el método de descenso de gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abded42a",
   "metadata": {},
   "source": [
    "3. Sea la función f : R^2 → R dada por\n",
    "\n",
    "##### g(x,y) = x2 + y3 + 3xy + 1\n",
    "\n",
    "---\n",
    "\n",
    "i) Aplica el método de descenso de gradiente sobre la función g(x, y) con los siguientes parámetros:\n",
    "\n",
    "Punto inicial: x0 = (-1, 1)\n",
    "\n",
    "Ratio de aprendizaje: gamma = 0.01\n",
    "\n",
    "Tolerancia: tol = 1e-12\n",
    "\n",
    "Número máximo de iteraciones: maxit = 1e5\n",
    "\n",
    "---\n",
    "\n",
    "ii) ¿Qué ocurre si ahora partimos de x0 = (0, 0)? ¿Se obtiene un resultado deseable?\n",
    "\n",
    "---\n",
    "\n",
    "iii) Realícese el estudio analítico de la función y utilícese para explicar y contrastar los resultados obtenidos en los dos apartados anteriores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c22563f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i) mínimo en x = [-2.25  1.5 ] tras 3140 iteraciones, g(x) = -0.6875000000\n",
      "ii) mínimo en x = [0. 0.] tras 1 iteraciones, g(x) = 1.0000000000\n",
      "\n",
      "Estudio analítico:\n",
      "Puntos críticos cumplen:\n",
      "2x + 3y = 0\n",
      "3y^2 + 3x = 0\n",
      "\n",
      "De la primera: x = -3/2 y\n",
      "Sustituyendo en la segunda:\n",
      "3 y^2 + 3 (-3/2 y) = 3 y^2 - (9/2) y = 0\n",
      "3 y (y - 3/2) = 0\n",
      "\n",
      "Por tanto, y=0 o y=3/2\n",
      "Si y=0 -> x=0\n",
      "Si y=3/2 -> x = -9/4 = -2.25\n",
      "\n",
      "Puntos críticos: (0,0) y (-2.25, 1.5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#volvemos a realizar la funcion descenso de gradiente (se podría usar la anterior)\n",
    "def descenso_gradiente_ej3(grad_f, x0, gamma, tol, maxit):\n",
    "    x = np.array(x0, dtype=float)\n",
    "    for i in range(maxit):\n",
    "        grad = grad_f(x)\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "        x = x - gamma * grad\n",
    "    return x, i+1\n",
    "\n",
    "# Función g(x,y)\n",
    "def g(x):\n",
    "    return x[0]**2 + x[1]**3 + 3*x[0]*x[1] + 1\n",
    "\n",
    "# Gradiente de g\n",
    "def grad_g(x):\n",
    "    dx = 2*x[0] + 3*x[1]\n",
    "    dy = 3*x[1]**2 + 3*x[0]\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "# Parámetros\n",
    "gamma = 0.01\n",
    "tol = 1e-12\n",
    "maxit = 1e5\n",
    "\n",
    "# i) Punto inicial (-1,1)\n",
    "x0_i = [-1, 1]\n",
    "x_min_i, iter_i = descenso_gradiente_ej3(grad_g, x0_i, gamma, tol, int(maxit))\n",
    "print(f\"i) mínimo en x = {x_min_i} tras {iter_i} iteraciones, g(x) = {g(x_min_i):.10f}\")\n",
    "\n",
    "# ii) Punto inicial (0,0)\n",
    "x0_ii = [0, 0]\n",
    "x_min_ii, iter_ii = descenso_gradiente_ej3(grad_g, x0_ii, gamma, tol, int(maxit))\n",
    "print(f\"ii) mínimo en x = {x_min_ii} tras {iter_ii} iteraciones, g(x) = {g(x_min_ii):.10f}\")\n",
    "\n",
    "# iii) Estudio analítico\n",
    "print(\"\"\"\n",
    "Estudio analítico:\n",
    "Puntos críticos cumplen:\n",
    "2x + 3y = 0\n",
    "3y^2 + 3x = 0\n",
    "\n",
    "De la primera: x = -3/2 y\n",
    "Sustituyendo en la segunda:\n",
    "3 y^2 + 3 (-3/2 y) = 3 y^2 - (9/2) y = 0\n",
    "3 y (y - 3/2) = 0\n",
    "\n",
    "Por tanto, y=0 o y=3/2\n",
    "Si y=0 -> x=0\n",
    "Si y=3/2 -> x = -9/4 = -2.25\n",
    "\n",
    "Puntos críticos: (0,0) y (-2.25, 1.5)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249c9088",
   "metadata": {},
   "source": [
    "### Interpretación\n",
    "\n",
    "- En el primer caso, partiendo de \\((-1,1)\\), el método converge a un mínimo local, indicando que el punto inicial está dentro de la zona de atracción de dicho mínimo.\n",
    "- En el segundo caso, partiendo de \\((0,0)\\), el método puede converger a un punto estacionario que no es mínimo (puede ser un punto de silla o máximo) o no converger correctamente, ya que la derivada es cero pero no se trata de un mínimo.\n",
    "- Por ello, la elección del punto inicial es fundamental para el éxito del método de descenso de gradiente. Además, el ratio de aprendizaje influye en la velocidad y estabilidad de la convergencia, como se ha observado en ejercicios anteriores.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
